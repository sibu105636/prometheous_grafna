apiVersion: v1
kind: Service
metadata:
  name: monitoring
spec:
  selector:
    app: monitoring
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
      nodePort: 30000
  type: NodePort
---
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: allow-external-access
spec:
  ingress:
  - ports:
    - port: 8000
      protocol: TCP
  podSelector:
    matchLabels:
      app: monitoring
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: monitoring
spec:
  replicas: 1 
  template:
    metadata:
      labels:
        app: monitoring
    spec:
      securityContext:
        runAsNonRoot: true
        fsGroup: 997
      containers:
      - name: traefik
        securityContext:
          runAsUser: 99
        image: reg-dhc.app.corpintra.net/example/traefik:1.3.6
        args:
        - --file
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: traefik-config
          mountPath: /etc/traefik
      - name: prometheus
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
        #image: reg-dhc.app.corpintra.net/example/prometheus:v1.2.1
        image: reg-dhc.app.corpintra.net/example/prometheus:v2.0.0
        args:
          - "--config.file=/etc/prometheus/prometheus.yaml"
          - "--storage.tsdb.retention=30d"
         # - "-storage.local.retention=730h"
         # - "-storage.local.series-file-shrink-ratio=30%"
         # - "-storage.local.memory-chunks=1048576"
        ports:
        - name: web
          containerPort: 9090
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-rules
          mountPath: /etc/alerts
        - name: prometheus-data
          mountPath: /prometheus/data
      - name: grafana
        securityContext:
          runAsNonRoot: true
          runAsUser: 472
#        image: reg-dhc.app.corpintra.net/eedc_a_sakrish/grafana:5.0.0
#        image: reg-dhc.app.corpintra.net/mathaku/grafana:v1.0
        #image: reg-dhc.app.corpintra.net/eedc_a_sakrish/grafana:latest
        #image: reg-dhc.app.corpintra.net/mathaku/grafana:new-proxy
        image: reg-dhc.app.corpintra.net/eedc_s_dhcregp/grafana:latest-apac-changes
        #image: reg-dhc.app.corpintra.net/mathaku/grafana:latest-apac-changes
        imagePullPolicy: Always
        ports:
        - name: grafana-http
          containerPort: 3000
        volumeMounts:
       # - name: grafana-config
       #   mountPath: /etc/grafana/grafana.ini
        - name: grafana-log
          mountPath: /usr/share/grafana/data/log
        - name: grafana-data
          mountPath: /usr/share/grafana/data
      - name: blackbox
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
        image: reg-dhc.app.corpintra.net/example/blackbox-exporter:v0.3.0
        args:
          - '-config.file=/etc/blackbox/blackbox.yaml'
        ports:
        - name: blackbox
          containerPort: 9115
        volumeMounts:
        - name: blackbox-config
          mountPath: /etc/blackbox
      - name: alertmanager
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
        image: reg-dhc.app.corpintra.net/example/alertmanager:v0.7.1
        env:
        - name: http_proxy
          value: "http:////53.18.255.131:3128"
        - name: https_proxy
          value: "http://53.18.255.131:3128"
        args:
          - '-config.file=/etc/alertmanager/alertmanager.yaml'
          - "--web.external-url=http://localhost/alertmanager/"
          - "-storage.path=/alertmanager"
        ports:
        - name: alertmanager
          containerPort: 9093
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        - name: alertmanager-data
          mountPath: /alertmanager
      volumes:
      - name: traefik-config
        configMap:
          name: traefik-config
      - name: prometheus-config
        configMap:
          name: prometheus
      - name: prometheus-rules
        configMap:
          name: prometheus-cm-rules
      - name: prometheus-data
        persistentVolumeClaim:
          claimName: data-persistence-disk
#      - name: grafana-config
#        configMap:
#          name: grafana-configmap
      - name: grafana-log
        emptyDir: {}
      - name: grafana-data
        emptyDir: {}
      - name: blackbox-config
        configMap:
          name: blackbox-exporter
      - name: alertmanager-config
        configMap:
          name: alertmanager
      - name: alertmanager-data
        emptyDir: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: traefik-config
data:
  traefik.toml: |
    debug = true
    checkNewVersion = false
    defaultEntryPoints = ["http"]
    [accessLog]
    [entryPoints]
      [entryPoints.http]
      address = ":8000"
    [frontends]
      [frontends.prometheus]
      backend = "prometheus"
      [frontends.grafana]
      backend = "grafana"
      passHostHeader = true
        [frontends.grafana.routes.ui]
        rule = "PathPrefixStrip:/grafana"
      [frontends.alertmanager]
      backend = "alertmanager"
      passHostHeader = true
        [frontends.alertmanager.routes.ui]
        rule = "PathPrefix:/alertmanager"
    [backends]
      [backends.grafana]
        [backends.grafana.servers.server1]
        url = "http://localhost:3000"
      [backends.prometheus]
        [backends.prometheus.servers.server1]
        url = "http://localhost:9090"
      [backends.alertmanager]
        [backends.alertmanager.servers.server1]
        url = "http://localhost:9093"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus
data:
  prometheus.yaml: |-
    global:
      scrape_interval: 10s
      external_labels:
        project: 'dhc02'
        datacenter: 'tss-emea'
    rule_files:
      - '/etc/alerts/prometheus.rules.yml'

    scrape_configs:
      - job_name: 'prometheus'
        # Override the global default and scrape targets from this job every 5 seconds.
        scrape_interval: 5s
        scrape_timeout: 5s
        metrics_path: /metrics
        static_configs:
          - targets:
            - 'localhost:9090'

      - job_name: 'quay-prod-health-check'
        scrape_interval: 5s
        metrics_path: /metrics
        static_configs:
          - targets:
               - 's96dhcvm04.destr.corpintra.net:9191'


      - job_name: 'quay_prod_sanity'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
               - 's96dhcvm04.destr.corpintra.net:9192'

      - job_name: 'quay_prod_sanity_apac'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
               - 'IADCA0000027.adc.corpintra.net:9192'

      - job_name: 'bb-prom-prod'
        scrape_interval: 30s
        #evaluation_interval: 15s 
        scheme: https #change to http if don't you have https
        tls_config:
             insecure_skip_verify: true
        metrics_path: '/plugins/servlet/prometheus/metrics'
        params:
             token: ['production']
        static_configs:
          - targets: ['git-dhc.app.corpintra.net']   
      - job_name: 'quay-prom-int'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.167.229:9092'
            - '53.18.167.230:9092'
            - '53.18.231.44:9092'
            - '53.18.231.45:9092'
      - job_name: 'EDC-QUAY-PD-sedcadhc0020'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.167.228:9100'
      - job_name: 'EDC_QUAY_PD-sedcadhc0010'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.167.227:9100'
      - job_name: 'APAC-QUAY-PD-sadca0000440'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.125.25.45:9100'
      - job_name: 'APAC-QUAY-PD-sadca0000441'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.125.25.46:9100'
      - job_name: 'EDC-DB-PD-sedcbdhc0010'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.169.63:9100'
      - job_name: 'EDC-DB-PD-sedcbdhc0020'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.169.64:9100'
      - job_name: 'EDC-DB-INT-sedcbdhc0060'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.169.66:9100'
      - job_name: 'EDC-DB-INT-sedcbdhc0050'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.169.65:9100'
      - job_name: 'EDC-QUAY-INT-sedcadhc0060'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.167.230:9100'
      - job_name: 'EDC-QUAY-INT-sedcadhc0050'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.167.229:9100'
      - job_name: 'quay-prom-prod'
        scrape_interval: 30s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '53.18.167.227:9092'
            - '53.18.167.228:9092'
            - '53.18.231.42:9092'
            - '53.18.231.43:9092'
    alerting:
      alertmanagers:
      - scheme: http
        path_prefix: "/alertmanager"
        static_configs:
        - targets:
          - "localhost:9093" 
#---
#apiVersion: v1
#kind: ConfigMap
#metadata:
#  name: grafana-configmap
#data:
#   grafana.ini: |-
#      [server]
#      root_url = %(protocol)s://%(domain)s:/grafana
#      [paths]
#      data = /var/lib/grafana
#      logs = /var/log/grafana
#      plugins = /var/lib/grafana/plugins
#      [dashboards.json]
#      enabled = true
#      path = /var/lib/grafana/dashboards/
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-cm-rules
data:
  prometheus.rules.yml: |-
      groups:
        - name: recording_rules
          interval: 5s
          rules:
            - record: node_exporter:node_filesystem_free:fs_used_percents
              expr: 100 - 100 * ( node_filesystem_free{mountpoint="/"} / node_filesystem_size{mountpoint="/"} )

            - record: node_exporter:node_memory_free:memory_used_percents
              expr: 100 - 100 * (node_memory_MemFree / node_memory_MemTotal)

        - name: LoadAverage15m
          rules:
            - alert: LoadAverage15m
              expr: node_load15 >= 1.75
              labels:
                service: SystemOSAlert
                severity: major
              annotations:
                summary: "Instance {{ $labels.instance }} - high load average"
                description: "{{ $labels.instance  }} (measured by {{ $labels.job }}) has high load average ({{ $value }}) over 15 minutes."
        - name: MemoryFree10%
          rules:
            - alert: MemoryFree10%
              expr: node_exporter:node_memory_free:memory_used_percents >= 90
              labels:
                service: SystemOSAlert
                severity: critical
              annotations:
                summary: "Instance {{ $labels.instance }} hight memory usage"
                description: "{{ $labels.instance }} has more than 90% of its memory used."
        - name: DiskSpace10%Free
          rules:
            - alert: DiskSpace10%Free
              expr: node_exporter:node_filesystem_free:fs_used_percents >= 90
              labels:
                service: SystemOSAlert
                severity: moderate
              annotations:
                summary: "Instance {{ $labels.instance }} is low on disk space"
                description: "{{ $labels.instance }} has only {{ $value }}% free."
        - name: ADCAlert
          rules:
            - alert: ADCQuaySanityCheck
              expr: adc_QuayServices_status < 1
              labels:
                service: QuaySanitycheck
                severity: major
              annotations:
                summary: "Instance {{ $labels.instance }} ADC is down"
                description: "{{ $labels.instance }} Adc service is down"
        - name: NAFTAAlert
          rules:
            - alert: NAFTASanityCheck
              expr: count(nafta_QuayServices_status < 1) >0
              labels:
                service: QuaySanitycheck
                severity: major
              annotations:
                summary: "Instance {{ $labels.instance }} NAFTA is down"
                description: "{{ $labels.instance }} NAFTA service is down"
        - name: EMEAAlert
          rules:
            - alert: EMEASanityCheck
              expr: count(app_QuayServices_status < 1) >0
              labels:
                service: QuaySanitycheck
                severity: major
              annotations:
                summary: "EMEA is down"
                description: "EMEA service is down"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: blackbox-exporter
data:
  blackbox.yaml: |-
    modules:
      http_2xx:
        prober: http
        timeout: 3s
        http:
          method: GET
          preferred_ip_protocol: "ip4"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager
data:
  alertmanager.yaml: |-
    global:
      resolve_timeout: 5m
    route:
      group_by: [ 'alertname', 'cluster', 'service' ]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 5h
      receiver: LoadAverage15m
      routes:
       - receiver: 'EMEASanityCheck'
         group_wait: 3m
         group_interval: 60m
         repeat_interval: 3h
         match_re:
          service: QuaySanitycheck
       - receiver: 'NAFTASanityCheck'
         group_wait: 3m
         group_interval: 60m
         repeat_interval: 3h
         match_re:
          service: QuaySanitycheck
       - receiver: 'NAFTASanityCheck'
         group_wait: 3m
         group_interval: 60m
         repeat_interval: 3h
         match_re:
          service: QuaySanitycheck
       - receiver: 'DiskSpace10%Free'
         group_wait: 3m
         group_interval: 30s
         repeat_interval: 3h
         match_re:
          service: SystemOSAlert
       - receiver: 'MemoryFree10%'
         group_wait: 30s
         group_interval: 30s
         repeat_interval: 3h
         match_re:
          service: SystemOSAlert
       - receiver: 'LoadAverage15m'
         group_wait: 30s
         group_interval: 30s
         repeat_interval: 3h
         match_re:
          service: SystemOSAlert
    receivers:
       - name: EMEASanityCheck
         pagerduty_configs:
          - service_key: 9f9e56b42b8a4b66b48b9f6d04915284 
       - name: NAFTASanityCheck
         pagerduty_configs:
          - service_key: 9f9e56b42b8a4b66b48b9f6d04915284
       - name: DiskSpace10%Free
         pagerduty_configs:
          - service_key: 9f9e56b42b8a4b66b48b9f6d04915284
       - name: MemoryFree10%
         pagerduty_configs:
          - service_key: 9f9e56b42b8a4b66b48b9f6d04915284
       - name: LoadAverage15m
         pagerduty_configs:
          - service_key: 9f9e56b42b8a4b66b48b9f6d04915284
